{
	"name": "CognitiveServices_NamedEntityRecog",
	"properties": {
		"folder": {
			"name": "NLP"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SparkPoolTest",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "ce1c3f6d-d355-4789-9593-bd47ff0329da"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolTest",
				"name": "SparkPoolTest",
				"type": "Spark",
				"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolTest",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# Add required imports\r\n",
					"import com.microsoft.spark.sqlanalytics\r\n",
					"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
					"from pyspark.sql.functions import col"
				],
				"execution_count": 65
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def QueryDataSQLPool (query):\r\n",
					"    # Name of the SQL Dedicated Pool or database where to run the query\r\n",
					"    # Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
					"    spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
					"\r\n",
					"    # Read from a query\r\n",
					"    # Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
					"    df = (spark.read\r\n",
					"                        # Name of the SQL Dedicated Pool or database where to run the query\r\n",
					"                        # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
					"                        .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
					"                        # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
					"                        # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
					"                        .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
					"                        # Defaults to storage path defined in the runtime configurations\r\n",
					"                        .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
					"                        # query from which data will be read\r\n",
					"                        .option(Constants.QUERY, query)\r\n",
					"                        .synapsesql()\r\n",
					"    )\r\n",
					"    return(df)"
				],
				"execution_count": 74
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Select sample size for testing\r\n",
					"\r\n",
					"# Sort ascending by 'created_at'\r\n",
					"from pyspark.sql.functions import desc\r\n",
					"\r\n",
					"df = df.orderBy(desc(\"created_at\"))\r\n",
					"\r\n",
					"# select the first 100 records\r\n",
					"#df = df.limit(100)"
				],
				"execution_count": 66
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import synapse.ml\r\n",
					"from synapse.ml.cognitive import *\r\n",
					"from pyspark.sql.functions import col"
				],
				"execution_count": 67
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"linked_service_name = \"CognitiveService1\""
				],
				"execution_count": 68
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Create function to run NER and output dataframe"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# import required libraries\r\n",
					"import json\r\n",
					"from pyspark.sql.types import *\r\n",
					"import pyspark.sql.functions as F\r\n",
					"from pyspark.sql.functions import lit"
				],
				"execution_count": 78
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"def NamedEntityRecognition(dataframe, batchsize, topic):\r\n",
					"    if dataframe.count() > topic:\r\n",
					"\r\n",
					"        # Batch size\r\n",
					"        batch_size = batchsize\r\n",
					"\r\n",
					"        # First, count the total number of rows in the DataFrame\r\n",
					"        num_rows = dataframe.count()\r\n",
					"\r\n",
					"        # Next, calculate the number of batches needed to split the DataFrame into batches of 5\r\n",
					"        num_batches = int(num_rows / batch_size) + (num_rows % batch_size > 0)\r\n",
					"\r\n",
					"        # Then, use randomSplit to split the DataFrame into smaller DataFrames of equal size\r\n",
					"        df_batches = dataframe.randomSplit([1.0]*num_batches, seed=42)\r\n",
					"\r\n",
					"        # define the schema for the output DataFrame\r\n",
					"        schema = StructType([\r\n",
					"            StructField(\"entity_text\", StringType(), True),\r\n",
					"            StructField(\"entity_category\", StringType(), True),\r\n",
					"            StructField(\"topic\", IntegerType(),True)\r\n",
					"            ])\r\n",
					"\r\n",
					"        # create an empty DataFrame with the defined schema\r\n",
					"        output_df = spark.createDataFrame([], schema)\r\n",
					"\r\n",
					"        entities = []\r\n",
					"        categories = []\r\n",
					"\r\n",
					"        # Finally, use limit to limit the number of rows in each DataFrame to 5\r\n",
					"        for i in range(num_batches):\r\n",
					"            batch = df_batches[i].limit(batch_size)\r\n",
					"            ner = (NER()\r\n",
					"            .setLinkedService(linked_service_name)\r\n",
					"            .setTextCol(\"cleantext\")\r\n",
					"            .setLanguageCol(\"lang\")\r\n",
					"            .setOutputCol(\"replies\")\r\n",
					"            .setErrorCol(\"error\"))\r\n",
					"\r\n",
					"            ner_batch_output = ner.transform(batch)\r\n",
					"            #display(ner_batch_output)\r\n",
					"\r\n",
					"            # Extract the 'replies' column and convert it to JSON\r\n",
					"            json_output = ner_batch_output.select(\"replies\").toJSON().collect()\r\n",
					"\r\n",
					"            # Deserialize the JSON and extract the 'replies' field\r\n",
					"            nes = [json.loads(x)[\"replies\"] for x in json_output]\r\n",
					"\r\n",
					"            # Print the 'replies' field for each row\r\n",
					"            for each in nes:\r\n",
					"                #print(each)\r\n",
					"                # convert the dictionary to a JSON string\r\n",
					"                json_str = json.dumps(each)\r\n",
					"\r\n",
					"                # parse the JSON string\r\n",
					"                json_data = json.loads(json_str)\r\n",
					"\r\n",
					"                # extract all the 'text' values from the JSON and put them into an array\r\n",
					"                entities = entities + [entity['text'] for entity in json_data['document']['entities']]\r\n",
					"                categories = categories + [entity['category'] for entity in json_data['document']['entities']]\r\n",
					"\r\n",
					"\r\n",
					"        # create rows\r\n",
					"        rows = [Row(entity_text=entities[i],\r\n",
					"                    entity_category=categories[i],\r\n",
					"                    topic = topic\r\n",
					"                    )\r\n",
					"                for i in range(len(entities))]\r\n",
					"\r\n",
					"        # create batch dataframe\r\n",
					"        ner_df = spark.createDataFrame(rows, schema)\r\n",
					"        return(ner_df)"
				],
				"execution_count": 90
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Topic 0 NER"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_topic0 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 0\")\r\n",
					"df_topic0.count()"
				],
				"execution_count": 91
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"topic0_NER = NamedEntityRecognition(dataframe = df_topic0, batchsize = 5, topic = 0)"
				],
				"execution_count": 92
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Topic 1 NER"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_topic1 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 1\")\r\n",
					"df_topic1.count()"
				],
				"execution_count": 93
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"topic1_NER = NamedEntityRecognition(dataframe = df_topic1, batchsize = 5, topic = 1)"
				],
				"execution_count": 94
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Topic 2 NER"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_topic2 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 2\")\r\n",
					"df_topic2.count()"
				],
				"execution_count": 95
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"topic2_NER = NamedEntityRecognition(dataframe = df_topic2, batchsize = 5, topic = 2)"
				],
				"execution_count": 96
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Topic 3 NER"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_topic3 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 3\")\r\n",
					"df_topic3.count()"
				],
				"execution_count": 97
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"topic3_NER = NamedEntityRecognition(dataframe = df_topic3, batchsize = 5, topic = 3)"
				],
				"execution_count": 98
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Topic 4 NER"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_topic4 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 4\")\r\n",
					"df_topic4.count()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"topic4_NER = NamedEntityRecognition(dataframe = df_topic4, batchsize = 5, topic = 4)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Union all Topic NER dataframes together"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# union the dataframes\r\n",
					"NameEntity_df = topic0_NER.union(topic1_NER).union(topic2_NER).union(topic3_NER).union(topic4_NER)\r\n",
					"display(NameEntity_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"NameEntity_df = topic0_NER.union(topic1_NER).union(topic2_NER)"
				],
				"execution_count": 100
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(NameEntity_df)"
				],
				"execution_count": 101
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Write Spark Dataframe to Dedicated SQL Pool Table [NER].[NATO_NER]\r\n",
					"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
					"~~~\r\n",
					"CREATE TABLE [NER].[NATO_NER]\r\n",
					"(\r\n",
					"    [entity_text] NVARCHAR(100) NULL,\r\n",
					"    [entity_category] NVARCHAR(100) NULL,\r\n",
					"    [topic] INT NULL\r\n",
					")\r\n",
					"\r\n",
					"GO"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Write using AAD Auth to internal table\r\n",
					"# Add required imports\r\n",
					"import com.microsoft.spark.sqlanalytics\r\n",
					"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
					"\r\n",
					"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
					"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
					"(NameEntity_df.write\r\n",
					" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
					" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
					" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
					" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
					" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
					" # Choose a save mode that is apt for your use case.\r\n",
					" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
					" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
					" .mode(\"overwrite\")\r\n",
					" # Required parameter - Three-part table name to which data will be written\r\n",
					" .synapsesql(\"SQLPoolTest.NER.NATO_NER\"))"
				],
				"execution_count": 102
			}
		]
	}
}