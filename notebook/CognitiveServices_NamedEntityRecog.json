{
	"name": "CognitiveServices_NamedEntityRecog",
	"properties": {
		"folder": {
			"name": "NLP"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SparkPoolSmall",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "90f5f6e8-8515-4172-a037-103daf078550"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
				"name": "SparkPoolSmall",
				"type": "Spark",
				"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# Add required imports\r\n",
					"import com.microsoft.spark.sqlanalytics\r\n",
					"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
					"from pyspark.sql.functions import col\r\n",
					"import synapse.ml\r\n",
					"from synapse.ml.cognitive import *"
				],
				"execution_count": 129
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"pip install azure-ai-textanalytics==5.2.0"
				],
				"execution_count": 130
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def QueryDataSQLPool (query):\r\n",
					"    # Name of the SQL Dedicated Pool or database where to run the query\r\n",
					"    # Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
					"    spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
					"\r\n",
					"    # Read from a query\r\n",
					"    # Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
					"    df = (spark.read\r\n",
					"                        # Name of the SQL Dedicated Pool or database where to run the query\r\n",
					"                        # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
					"                        .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
					"                        # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
					"                        # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
					"                        .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
					"                        # Defaults to storage path defined in the runtime configurations\r\n",
					"                        .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
					"                        # query from which data will be read\r\n",
					"                        .option(Constants.QUERY, query)\r\n",
					"                        .synapsesql()\r\n",
					"    )\r\n",
					"    return(df)"
				],
				"execution_count": 131
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Enter Cognitive Services credentials\r\n",
					"key = \"a3028b5f7e7a462d8a3381e49b9c976d\"\r\n",
					"endpoint = \"https://pendragon-language.cognitiveservices.azure.com/\"\r\n",
					"\r\n",
					"from azure.ai.textanalytics import TextAnalyticsClient\r\n",
					"from azure.core.credentials import AzureKeyCredential\r\n",
					"\r\n",
					"# Authenticate the client using your key and endpoint \r\n",
					"def authenticate_client():\r\n",
					"    ta_credential = AzureKeyCredential(key)\r\n",
					"    text_analytics_client = TextAnalyticsClient(\r\n",
					"            endpoint=endpoint, \r\n",
					"            credential=ta_credential)\r\n",
					"    return text_analytics_client\r\n",
					"\r\n",
					"client = authenticate_client()"
				],
				"execution_count": 132
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Example function for recognizing entities from text\r\n",
					"def entity_recognition(client,doc):\r\n",
					"\r\n",
					"    try:\r\n",
					"        documents = doc\r\n",
					"        result = client.recognize_entities(documents = documents)[0]\r\n",
					"\r\n",
					"        entities =[]\r\n",
					"        categories = []\r\n",
					"        subcategories = []\r\n",
					"        confidencescores = []\r\n",
					"        entity_lengths = []\r\n",
					"        entity_offsets = []\r\n",
					"\r\n",
					"        #print(\"Named Entities:\\n\")\r\n",
					"        for entity in result.entities:\r\n",
					"            entities.append(entity.text)\r\n",
					"            categories.append(entity.category)\r\n",
					"            subcategories.append(entity.subcategory)\r\n",
					"            confidencescores.append(entity.confidence_score)\r\n",
					"            entity_lengths.append(entity.length)\r\n",
					"            entity_offsets.append(entity.offset)\r\n",
					"\r\n",
					"    except Exception as err:\r\n",
					"        print(\"Encountered exception. {}\".format(err))\r\n",
					"    \r\n",
					"    return(entities, categories, subcategories, confidencescores, entity_lengths, entity_offsets)"
				],
				"execution_count": 133
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"linked_service_name = \"CognitiveService1\""
				],
				"execution_count": 134
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Create function to run NER and output dataframe"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# import required libraries\r\n",
					"import json\r\n",
					"from pyspark.sql.types import *\r\n",
					"import pyspark.sql.functions as F\r\n",
					"from pyspark.sql.functions import lit"
				],
				"execution_count": 135
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create function to split input to Cogntive Services Extractive Summarization into batches of 125000 characters max\r\n",
					"def split_into_batches(input_string, max_chars_per_batch):\r\n",
					"    current_batch = \"\"\r\n",
					"    result = []\r\n",
					"    for c in input_string:\r\n",
					"        if len(current_batch) + 1 > max_chars_per_batch:\r\n",
					"            result.append(current_batch)\r\n",
					"            current_batch = c\r\n",
					"        else:\r\n",
					"            current_batch += c\r\n",
					"    if current_batch:\r\n",
					"        result.append(current_batch)\r\n",
					"    return result"
				],
				"execution_count": 136
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pandas as pd\r\n",
					"from pyspark.sql.functions import pandas_udf, col\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType, ArrayType"
				],
				"execution_count": 137
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def NamedEntityRecognition(dataframe, topic):\r\n",
					"    if dataframe.count() > 1:\r\n",
					"        text_col = dataframe.select('cleantext')\r\n",
					"        # Collect all tweets into a list\r\n",
					"        text_list = text_col.rdd.map(lambda row: row[0]).collect()\r\n",
					"        # Join all tweets into a single document\r\n",
					"        result = \" \".join([str(text) + \".\" for text in text_list])\r\n",
					"        document = [result]\r\n",
					"        # Extract the string that contains all tweets\r\n",
					"        input_string = document[0]\r\n",
					"\r\n",
					"        batches = split_into_batches(input_string, 5120)\r\n",
					"        num_batches = len(batches)\r\n",
					"        print(num_batches)\r\n",
					"\r\n",
					"        schema = StructType([\r\n",
					"            StructField(\"entity\",  StringType(), True),\r\n",
					"            StructField(\"entity_category\", StringType(), True),\r\n",
					"            StructField(\"entity_subcategory\", StringType(), True),\r\n",
					"            StructField(\"topic\", ShortType(), True)\r\n",
					"        ])\r\n",
					"\r\n",
					"        # create a DataFrame\r\n",
					"        ner_output_df =  spark.createDataFrame([], schema)\r\n",
					"\r\n",
					"        # Finally, use limit to limit the number of rows in each DataFrame to 5\r\n",
					"        for each in batches:\r\n",
					"            doc = [each]\r\n",
					"            entities, categories, subcategories, confidencescores, entity_lengths, entity_offsets = entity_recognition(client, doc)\r\n",
					"            # combine data into a list of rows\r\n",
					"            data = [(a1, a2, a3, topic) for a1, a2, a3 in zip(entities, categories, subcategories)]\r\n",
					"            batch_output_df = spark.createDataFrame([Row(*i) for i in data], schema)\r\n",
					"            #print(batch_output_df.count())\r\n",
					"\r\n",
					"            ner_output_df = ner_output_df.union(batch_output_df)\r\n",
					"        return ner_output_df\r\n",
					"\r\n",
					"    else:\r\n",
					"        return(spark.createDataFrame(['No Data','No Data','No Data', topic], schema)) \r\n",
					""
				],
				"execution_count": 138
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_all = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment\")\r\n",
					"df_all.count()"
				],
				"execution_count": 139
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df_all_ner = NamedEntityRecognition(df_all,5)"
				],
				"execution_count": 140
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Topic 0 NER"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_topic0 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 0\")\r\n",
					"df_topic0.count()"
				],
				"execution_count": 142
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"topic0_NER = NamedEntityRecognition(dataframe = df_topic0, topic = 0)"
				],
				"execution_count": 143
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Topic 1 NER"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_topic1 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 1\")\r\n",
					"df_topic1.count()"
				],
				"execution_count": 144
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"topic1_NER = NamedEntityRecognition(dataframe = df_topic1, topic = 1)"
				],
				"execution_count": 145
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Topic 2 NER"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_topic2 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 2\")\r\n",
					"df_topic2.count()"
				],
				"execution_count": 146
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"topic2_NER = NamedEntityRecognition(dataframe = df_topic2, topic = 2)"
				],
				"execution_count": 147
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Topic 3 NER"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_topic3 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 3\")\r\n",
					"df_topic3.count()"
				],
				"execution_count": 148
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"topic3_NER = NamedEntityRecognition(dataframe = df_topic3, topic = 3)"
				],
				"execution_count": 149
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Topic 4 NER"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_topic4 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 4\")\r\n",
					"df_topic4.count()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"topic4_NER = NamedEntityRecognition(dataframe = df_topic4, topic = 4)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Union all Topic NER dataframes together"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# union the dataframes\r\n",
					"NameEntity_df = topic0_NER.union(topic1_NER).union(topic2_NER).union(topic3_NER).union(topic4_NER)\r\n",
					"display(NameEntity_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(NameEntity_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Write Spark Dataframe to Dedicated SQL Pool Table [NER].[NATO_NER]\r\n",
					"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
					"~~~\r\n",
					"CREATE TABLE [NER].[NATO_NER]\r\n",
					"(\r\n",
					"    [entity_text] NVARCHAR(100) NULL,\r\n",
					"    [entity_category] NVARCHAR(100) NULL,\r\n",
					"    [topic] INT NULL\r\n",
					")\r\n",
					"\r\n",
					"GO"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Write using AAD Auth to internal table\r\n",
					"# Add required imports\r\n",
					"import com.microsoft.spark.sqlanalytics\r\n",
					"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
					"\r\n",
					"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
					"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
					"(NameEntity_df.write\r\n",
					" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
					" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
					" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
					" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
					" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
					" # Choose a save mode that is apt for your use case.\r\n",
					" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
					" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
					" .mode(\"overwrite\")\r\n",
					" # Required parameter - Three-part table name to which data will be written\r\n",
					" .synapsesql(\"SQLPoolTest.NER.NATO_NER\"))"
				],
				"execution_count": null
			}
		]
	}
}