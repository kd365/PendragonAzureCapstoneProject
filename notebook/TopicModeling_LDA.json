{
	"name": "TopicModeling_LDA",
	"properties": {
		"folder": {
			"name": "NLP"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SparkPoolTest",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "a60338e1-f9a4-45c0-bbe5-d1dc1167fd8d"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolTest",
				"name": "SparkPoolTest",
				"type": "Spark",
				"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolTest",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# Add required imports\r\n",
					"import com.microsoft.spark.sqlanalytics\r\n",
					"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
					"from pyspark.sql.functions import col\r\n",
					"\r\n",
					"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
					"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
					"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
					"\r\n",
					"# Read from a query\r\n",
					"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
					"df = (spark.read\r\n",
					"                     # Name of the SQL Dedicated Pool or database where to run the query\r\n",
					"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
					"                     .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
					"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
					"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
					"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
					"                     # Defaults to storage path defined in the runtime configurations\r\n",
					"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
					"                     # query from which data will be read\r\n",
					"                     .option(Constants.QUERY, \"select * from dbo.NATO_Tweets1\")\r\n",
					"                     .synapsesql()\r\n",
					")"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Select sample size for testing\r\n",
					"\r\n",
					"# Sort ascending by 'created_at'\r\n",
					"from pyspark.sql.functions import desc\r\n",
					"\r\n",
					"df = df.orderBy(desc(\"created_at\"))\r\n",
					"\r\n",
					"# select the first 100 records\r\n",
					"df = df.limit(100)\r\n",
					"display(df)"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"import re\r\n",
					"from pyspark.ml.feature import StopWordsRemover\r\n",
					"from pyspark.sql.functions import udf\r\n",
					"from pyspark.sql.types import StringType\r\n",
					"\r\n",
					"# Define regular expressions for special characters, URLs, hashtags, mentions, and emojis\r\n",
					"special_chars_regex = r'[^\\w\\s]'\r\n",
					"url_regex = r'https?://\\S+'\r\n",
					"hashtag_regex = r'#\\w+'\r\n",
					"mention_regex = r'@\\w+'\r\n",
					"emoji_regex = r'[^\\w\\s\\ufe0f]+'\r\n",
					"\r\n",
					"# Define a list of stop words\r\n",
					"stop_words = StopWordsRemover().getStopWords()\r\n",
					"\r\n",
					"# Define a UDF (user-defined function) to apply the regular expressions and stop words removal to each row of the dataframe\r\n",
					"def clean_text(text):\r\n",
					"    # Remove URLs, hashtags, mentions, and emojis using regular expressions\r\n",
					"    text = re.sub(url_regex, '', text)\r\n",
					"    text = re.sub(hashtag_regex, '', text)\r\n",
					"    text = re.sub(mention_regex, '', text)\r\n",
					"    text = re.sub(emoji_regex, '', text)\r\n",
					"\r\n",
					"    # Remove special characters and convert to lowercase\r\n",
					"    text = re.sub(special_chars_regex, '', text).lower()\r\n",
					"\r\n",
					"    # Remove stop words\r\n",
					"    words = text.split()\r\n",
					"    words = [w for w in words if w not in stop_words]\r\n",
					"    text = ' '.join(words)\r\n",
					"\r\n",
					"    return text\r\n",
					"\r\n",
					"# Define the UDF as a Spark SQL function\r\n",
					"from pyspark.sql.functions import udf\r\n",
					"from pyspark.sql.types import StringType\r\n",
					"clean_text_udf = udf(clean_text, StringType())\r\n",
					"\r\n",
					"# Apply the UDF to the 'text' column of the dataframe and store the result in a new column named 'cleanText'\r\n",
					"df = df.withColumn('cleanText', clean_text_udf('text'))\r\n",
					"display(df)"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.ml.feature import Tokenizer, CountVectorizer\r\n",
					"from pyspark.ml.clustering import LDA\r\n",
					"\r\n",
					"# tokenize text\r\n",
					"tokenizer = Tokenizer(inputCol='cleanText', outputCol='tokens')\r\n",
					"df_tokens = tokenizer.transform(df)\r\n",
					"\r\n",
					"# create document-term matrix\r\n",
					"vectorizer = CountVectorizer(inputCol='tokens', outputCol='features')\r\n",
					"model = vectorizer.fit(df_tokens)\r\n",
					"df_features = model.transform(df_tokens)\r\n",
					"\r\n",
					"# fit LDA model\r\n",
					"num_topics = 5\r\n",
					"max_iterations = 10\r\n",
					"lda = LDA(k=num_topics, maxIter=max_iterations)\r\n",
					"lda_model = lda.fit(df_features)\r\n",
					"\r\n",
					"# get topic distribution for each document\r\n",
					"df_topics = lda_model.transform(df_features).select('id', 'topicDistribution')\r\n",
					"\r\n",
					"# join with original DataFrame\r\n",
					"df_result = df.join(df_topics, 'id')\r\n",
					"\r\n",
					"display(df_result)"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import json\r\n",
					"import numpy as np\r\n",
					"\r\n",
					"# Extract the 'replies' column and convert it to JSON\r\n",
					"json_output = df_result.select(\"topicDistribution\").toJSON().collect()\r\n",
					"\r\n",
					"# Deserialize the JSON and extract the 'replies' field\r\n",
					"topics = [json.loads(x)[\"topicDistribution\"] for x in json_output]\r\n",
					"\r\n",
					"topicgroup = []\r\n",
					"topic0confidence = []\r\n",
					"topic1confidence = []\r\n",
					"topic2confidence = []\r\n",
					"topic3confidence = []\r\n",
					"topic4confidence = []\r\n",
					"\r\n",
					"# Print the 'replies' field for each row\r\n",
					"for topic in topics:\r\n",
					"    #print(topic)\r\n",
					"    \r\n",
					"    # convert JSON array to numpy array\r\n",
					"    arr = np.array(topic['values'])\r\n",
					"    topic0confidence.append(arr[0])\r\n",
					"    topic1confidence.append(arr[1])\r\n",
					"    topic2confidence.append(arr[2])\r\n",
					"    topic3confidence.append(arr[3])\r\n",
					"    topic4confidence.append(arr[4])\r\n",
					"\r\n",
					"    # find index of maximum value\r\n",
					"    idx = np.argmax(arr)\r\n",
					"    #print(idx)\r\n",
					"    topicgroup.append(idx)\r\n",
					"\r\n",
					"#print(topicgroup)\r\n",
					"#print(topic0confidence)\r\n",
					"#print(topic1confidence)\r\n",
					"#print(topic2confidence)\r\n",
					"#print(topic3confidence)\r\n",
					"#print(topic4confidence)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# assume df is the DataFrame to extract values from\r\n",
					"id_values = []\r\n",
					"created_at_values = []\r\n",
					"text_values = []\r\n",
					"cleantext_values = []\r\n",
					"lang_values = []\r\n",
					"retweet_count_values = []\r\n",
					"reply_count_values = []\r\n",
					"like_count_values = []\r\n",
					"quote_count_values = []\r\n",
					"impression_count_values = []\r\n",
					"\r\n",
					"# collect all rows of the DataFrame\r\n",
					"rows = df.collect()\r\n",
					"\r\n",
					"# iterate over rows and extract values from each column\r\n",
					"for row in rows:\r\n",
					"    id_values.append(row[\"id\"])\r\n",
					"    created_at_values.append(row[\"created_at\"])\r\n",
					"    text_values.append(row[\"text\"])\r\n",
					"    lang_values.append(row[\"lang\"])\r\n",
					"    retweet_count_values.append(row[\"retweet_count\"])\r\n",
					"    reply_count_values.append(row[\"reply_count\"])\r\n",
					"    like_count_values.append(row[\"like_count\"])\r\n",
					"    quote_count_values.append(row[\"quote_count\"])\r\n",
					"    impression_count_values.append(row[\"impression_count\"])\r\n",
					"    cleantext_values.append(row['cleanText'])"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Convert nump.int64 to int and numpy.float64 to float\r\n",
					"topicgroup = [int(i) for i in topicgroup]\r\n",
					"topic0confidence = [float(i) for i in topic0confidence]\r\n",
					"topic1confidence = [float(i) for i in topic1confidence]\r\n",
					"topic2confidence = [float(i) for i in topic2confidence]\r\n",
					"topic3confidence = [float(i) for i in topic3confidence]\r\n",
					"topic4confidence = [float(i) for i in topic4confidence]"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.types import *\r\n",
					"schema = StructType([\r\n",
					"    StructField(\"id\",  LongType(), True),\r\n",
					"    StructField(\"created_at\", TimestampType(), True),\r\n",
					"    StructField(\"text\", StringType(), True),\r\n",
					"    StructField(\"cleantext\", StringType(), True),\r\n",
					"    StructField(\"lang\", StringType(), True),\r\n",
					"    StructField(\"retweet_count\", IntegerType(), True),\r\n",
					"    StructField(\"reply_count\", IntegerType(), True),\r\n",
					"    StructField(\"like_count\", IntegerType(), True),\r\n",
					"    StructField(\"quote_count\", IntegerType(), True),\r\n",
					"    StructField(\"impression_count\", IntegerType(), True),\r\n",
					"    StructField(\"topic\", ShortType(), True),\r\n",
					"    StructField(\"topic_0_confidence\", FloatType(), True),\r\n",
					"    StructField(\"topic_1_confidence\", FloatType(), True),\r\n",
					"    StructField(\"topic_2_confidence\", FloatType(), True),\r\n",
					"    StructField(\"topic_3_confidence\", FloatType(), True),\r\n",
					"    StructField(\"topic_4_confidence\", FloatType(), True)\r\n",
					"])\r\n",
					"\r\n",
					"# create rows\r\n",
					"rows = [Row(id=id_values[i],\r\n",
					"            created_at=created_at_values[i],\r\n",
					"            text=text_values[i],\r\n",
					"            cleanText = cleantext_values[i],\r\n",
					"            lang=lang_values[i],\r\n",
					"            retweet_count = retweet_count_values[i],\r\n",
					"            reply_count=reply_count_values[i],\r\n",
					"            like_count=like_count_values[i],\r\n",
					"            quote_count=quote_count_values[i],\r\n",
					"            impression_count = impression_count_values[i],\r\n",
					"            topic = topicgroup[i],\r\n",
					"            topic_0_confidence = topic0confidence[i],\r\n",
					"            topic_1_confidence = topic1confidence[i],\r\n",
					"            topic_2_confidence = topic2confidence[i],\r\n",
					"            topic_3_confidence = topic3confidence[i],\r\n",
					"            topic_4_confidence = topic4confidence[i])\r\n",
					"        for i in range(len(id_values))]\r\n",
					"\r\n",
					"# create a DataFrame\r\n",
					"final_topic_df = spark.createDataFrame(rows, schema)\r\n",
					"\r\n",
					"# show the DataFrame\r\n",
					"display(final_topic_df)\r\n",
					""
				],
				"execution_count": 16
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Write Spark Dataframe to Dedicated SQL Pool Table [Topic].[NATO_Topics]\r\n",
					"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
					"~~~\r\n",
					"CREATE TABLE [Topic].[NATO_Topics]\r\n",
					"( \r\n",
					"\t[id] bigint  NULL,\r\n",
					"\t[created_at] DATETIME2(7)  NULL,\r\n",
					"\t[text] NVARCHAR(4000)  NULL,\r\n",
					"\t[cleantext] NVARCHAR(4000)  NULL,\r\n",
					"\t[lang] nvarchar(10)  NULL,\r\n",
					"\t[retweet_count] INT  NULL,\r\n",
					"\t[reply_count] INT  NULL,\r\n",
					"\t[like_count] INT  NULL,\r\n",
					"\t[quote_count] INT  NULL,\r\n",
					"\t[impression_count] INT  NULL,\r\n",
					"    [topic] SMALLINT NULL,\r\n",
					"    [topic_0_confidence] REAL NULL,\r\n",
					"    [topic_1_confidence] REAL NULL,\r\n",
					"    [topic_2_confidence] REAL NULL,\r\n",
					"    [topic_3_confidence] REAL NULL,\r\n",
					"    [topic_4_confidence] REAL NULL\r\n",
					")\r\n",
					"\r\n",
					"GO"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Write using AAD Auth to internal table\r\n",
					"# Add required imports\r\n",
					"import com.microsoft.spark.sqlanalytics\r\n",
					"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
					"\r\n",
					"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
					"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
					"(final_topic_df.write\r\n",
					" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
					" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
					" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
					" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
					" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
					" # Choose a save mode that is apt for your use case.\r\n",
					" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
					" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
					" .mode(\"overwrite\")\r\n",
					" # Required parameter - Three-part table name to which data will be written\r\n",
					" .synapsesql(\"SQLPoolTest.Topic.NATO_Topics\"))"
				],
				"execution_count": 17
			}
		]
	}
}