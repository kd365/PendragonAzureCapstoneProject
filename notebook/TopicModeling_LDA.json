{
	"name": "TopicModeling_LDA",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SparkPoolTest",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "1fb4af3e-b23d-4c29-a770-864fe523a0cd"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolTest",
				"name": "SparkPoolTest",
				"type": "Spark",
				"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolTest",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# Add required imports\r\n",
					"import com.microsoft.spark.sqlanalytics\r\n",
					"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
					"from pyspark.sql.functions import col\r\n",
					"\r\n",
					"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
					"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
					"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
					"\r\n",
					"# Read from a query\r\n",
					"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
					"df = (spark.read\r\n",
					"                     # Name of the SQL Dedicated Pool or database where to run the query\r\n",
					"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
					"                     .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
					"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
					"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
					"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
					"                     # Defaults to storage path defined in the runtime configurations\r\n",
					"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
					"                     # query from which data will be read\r\n",
					"                     .option(Constants.QUERY, \"select * from dbo.NATO_Tweets1\")\r\n",
					"                     .synapsesql()\r\n",
					")"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Select sample size for testing\r\n",
					"\r\n",
					"# Sort ascending by 'created_at'\r\n",
					"from pyspark.sql.functions import desc\r\n",
					"\r\n",
					"df = df.orderBy(desc(\"created_at\"))\r\n",
					"\r\n",
					"# select the first 100 records\r\n",
					"df = df.limit(100)"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import regexp_replace\r\n",
					"\r\n",
					"# Define the regular expression pattern to match words starting with @, &, or https\r\n",
					"regex = r'@\\w+|&\\w+|https\\w+|#\\w+'\r\n",
					"\r\n",
					"# Apply the regexp_replace function to the text column and create a new column with the filtered text\r\n",
					"df = df.withColumn(\"text\", regexp_replace(col(\"text\"), regex, ''))\r\n",
					"\r\n",
					"# Show the filtered DataFrame\r\n",
					"df.select(\"text\").show()"
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.ml.feature import Tokenizer, CountVectorizer\r\n",
					"from pyspark.ml.clustering import LDA\r\n",
					"\r\n",
					"# tokenize text\r\n",
					"tokenizer = Tokenizer(inputCol='text', outputCol='tokens')\r\n",
					"df_tokens = tokenizer.transform(df)\r\n",
					"\r\n",
					"# create document-term matrix\r\n",
					"vectorizer = CountVectorizer(inputCol='tokens', outputCol='features')\r\n",
					"model = vectorizer.fit(df_tokens)\r\n",
					"df_features = model.transform(df_tokens)\r\n",
					"\r\n",
					"# fit LDA model\r\n",
					"num_topics = 5\r\n",
					"max_iterations = 10\r\n",
					"lda = LDA(k=num_topics, maxIter=max_iterations)\r\n",
					"lda_model = lda.fit(df_features)\r\n",
					"\r\n",
					"# get topic distribution for each document\r\n",
					"df_topics = lda_model.transform(df_features).select('id', 'topicDistribution')\r\n",
					"\r\n",
					"# join with original DataFrame\r\n",
					"df_result = df.join(df_topics, 'id')\r\n",
					"\r\n",
					"display(df_result)"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import json\r\n",
					"import numpy as np\r\n",
					"\r\n",
					"# Extract the 'replies' column and convert it to JSON\r\n",
					"json_output = df_result.select(\"topicDistribution\").toJSON().collect()\r\n",
					"\r\n",
					"# Deserialize the JSON and extract the 'replies' field\r\n",
					"topics = [json.loads(x)[\"topicDistribution\"] for x in json_output]\r\n",
					"\r\n",
					"topicgroup = []\r\n",
					"topic0confidence = []\r\n",
					"topic1confidence = []\r\n",
					"topic2confidence = []\r\n",
					"topic3confidence = []\r\n",
					"topic4confidence = []\r\n",
					"\r\n",
					"# Print the 'replies' field for each row\r\n",
					"for topic in topics:\r\n",
					"    #print(topic)\r\n",
					"    \r\n",
					"    # convert JSON array to numpy array\r\n",
					"    arr = np.array(topic['values'])\r\n",
					"    topic0confidence.append(arr[0])\r\n",
					"    topic1confidence.append(arr[1])\r\n",
					"    topic2confidence.append(arr[2])\r\n",
					"    topic3confidence.append(arr[3])\r\n",
					"    topic4confidence.append(arr[4])\r\n",
					"\r\n",
					"    # find index of maximum value\r\n",
					"    idx = np.argmax(arr)\r\n",
					"    #print(idx)\r\n",
					"    topicgroup.append(idx)\r\n",
					"\r\n",
					"print(topicgroup)\r\n",
					"print(topic0confidence)\r\n",
					"print(topic1confidence)\r\n",
					"print(topic2confidence)\r\n",
					"print(topic3confidence)\r\n",
					"print(topic4confidence)"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# assume df is the DataFrame to extract values from\r\n",
					"id_values = []\r\n",
					"created_at_values = []\r\n",
					"text_values = []\r\n",
					"lang_values = []\r\n",
					"retweet_count_values = []\r\n",
					"reply_count_values = []\r\n",
					"like_count_values = []\r\n",
					"quote_count_values = []\r\n",
					"impression_count_values = []\r\n",
					"\r\n",
					"# collect all rows of the DataFrame\r\n",
					"rows = df.collect()\r\n",
					"\r\n",
					"# iterate over rows and extract values from each column\r\n",
					"for row in rows:\r\n",
					"    id_values.append(row[\"id\"])\r\n",
					"    created_at_values.append(row[\"created_at\"])\r\n",
					"    text_values.append(row[\"text\"])\r\n",
					"    lang_values.append(row[\"lang\"])\r\n",
					"    retweet_count_values.append(row[\"retweet_count\"])\r\n",
					"    reply_count_values.append(row[\"reply_count\"])\r\n",
					"    like_count_values.append(row[\"like_count\"])\r\n",
					"    quote_count_values.append(row[\"quote_count\"])\r\n",
					"    impression_count_values.append(row[\"impression_count\"])"
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"print(type(topic0confidence[0]))"
				],
				"execution_count": 50
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Convert nump.int64 to int and numpy.float64 to float\r\n",
					"topicgroup = [int(i) for i in topicgroup]\r\n",
					"topic0confidence = [float(i) for i in topic0confidence]\r\n",
					"topic1confidence = [float(i) for i in topic1confidence]\r\n",
					"topic2confidence = [float(i) for i in topic2confidence]\r\n",
					"topic3confidence = [float(i) for i in topic3confidence]\r\n",
					"topic4confidence = [float(i) for i in topic4confidence]"
				],
				"execution_count": 51
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.types import *\r\n",
					"schema = StructType([\r\n",
					"    StructField(\"id\",  LongType(), True),\r\n",
					"    StructField(\"created_at\", TimestampType(), True),\r\n",
					"    StructField(\"text\", StringType(), True),\r\n",
					"    StructField(\"lang\", StringType(), True),\r\n",
					"    StructField(\"reply_count\", IntegerType(), True),\r\n",
					"    StructField(\"like_count\", IntegerType(), True),\r\n",
					"    StructField(\"quote_count\", IntegerType(), True),\r\n",
					"    StructField(\"impression_count\", IntegerType(), True),\r\n",
					"    StructField(\"Topic\", IntegerType(), True),\r\n",
					"    StructField(\"Topic 0 Confidence\", FloatType(), True),\r\n",
					"    StructField(\"Topic 1 Confidence\", FloatType(), True),\r\n",
					"    StructField(\"Topic 2 Confidence\", FloatType(), True),\r\n",
					"    StructField(\"Topic 3 Confidence\", FloatType(), True),\r\n",
					"    StructField(\"Topic 4 Confidence\", FloatType(), True)\r\n",
					"])\r\n",
					"\r\n",
					"# create rows\r\n",
					"rows = [Row(id=id_values[i],\r\n",
					"            created_at=created_at_values[i],\r\n",
					"            text=text_values[i],\r\n",
					"            lang=lang_values[i],\r\n",
					"            reply_count=reply_count_values[i],\r\n",
					"            like_count=like_count_values[i],\r\n",
					"            quote_count=quote_count_values[i],\r\n",
					"            impression_count=impression_count_values[i],\r\n",
					"            Topic=topicgroup[i],\r\n",
					"            topic0confidence=topic0confidence[i],\r\n",
					"            topic1confidence=topic1confidence[i],\r\n",
					"            topic2confidence=topic2confidence[i],\r\n",
					"            topic3confidence=topic3confidence[i],\r\n",
					"            topic4confidence=topic4confidence[i])\r\n",
					"        for i in range(len(id_values))]\r\n",
					"\r\n",
					"# create a DataFrame\r\n",
					"final_topic_df = spark.createDataFrame(rows, schema)\r\n",
					"\r\n",
					"# show the DataFrame\r\n",
					"#display(final_topic_df)\r\n",
					""
				],
				"execution_count": 54
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Write Spark Dataframe to Dedicated SQL Pool Table [Topic].[NATO_Topics]\r\n",
					"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
					"~~~\r\n",
					"CREATE TABLE [Topic].[NATO_Topics]\r\n",
					"( \r\n",
					"\t[id] bigint  NULL,\r\n",
					"\t[created_at] DATETIME2(7)  NULL,\r\n",
					"\t[text] NVARCHAR(4000)  NULL,\r\n",
					"\t[lang] nvarchar(10)  NULL,\r\n",
					"\t[retweet_count] INT  NULL,\r\n",
					"\t[reply_count] INT  NULL,\r\n",
					"\t[like_count] INT  NULL,\r\n",
					"\t[quote_count] INT  NULL,\r\n",
					"\t[impression_count] INT  NULL,\r\n",
					"    [Topic] INT NULL,\r\n",
					"    [Topic 0 Confidence] REAL NULL,\r\n",
					"    [Topic 1 Confidence] REAL NULL,\r\n",
					"    [Topic 2 Confidence] REAL NULL,\r\n",
					"    [Topic 3 Confidence] REAL NULL,\r\n",
					"    [Topic 4 Confidence] REAL NULL\r\n",
					")\r\n",
					"\r\n",
					"GO"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Write using AAD Auth to internal table\r\n",
					"# Add required imports\r\n",
					"import com.microsoft.spark.sqlanalytics\r\n",
					"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
					"\r\n",
					"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
					"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
					"(final_topic_df.write\r\n",
					" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
					" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
					" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
					" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
					" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
					" # Choose a save mode that is apt for your use case.\r\n",
					" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
					" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
					" .mode(\"overwrite\")\r\n",
					" # Required parameter - Three-part table name to which data will be written\r\n",
					" .synapsesql(\"SQLPoolTest.Topic.NATO_Topics\"))"
				],
				"execution_count": 55
			}
		]
	}
}