{
	"name": "CogntiveServices_Summarization",
	"properties": {
		"folder": {
			"name": "NLP"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SparkPoolTest",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "2798cebe-e6c4-4c4f-8e5e-2d53dbfcd377"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolTest",
				"name": "SparkPoolTest",
				"type": "Spark",
				"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolTest",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Cognitive Services Extractive Summary for inputs into OpenAI for Abstractive Summary \r\n",
					"## For Each topic\r\n",
					"**Possibly will fail if number of tweets in a topic group is less than 30 tweets**\r\n",
					"<hr>"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Install necessary packags and create functions"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"pip install azure-ai-textanalytics==5.3.0b1"
				],
				"execution_count": 315
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"!pip install openai"
				],
				"execution_count": 316
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Add required imports\r\n",
					"import com.microsoft.spark.sqlanalytics\r\n",
					"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
					"from pyspark.sql.functions import *\r\n",
					"from pyspark.sql.types import *\r\n",
					"import re\r\n",
					"from azure.ai.textanalytics import TextAnalyticsClient\r\n",
					"from azure.core.credentials import AzureKeyCredential\r\n",
					"from azure.core.credentials import AzureKeyCredential\r\n",
					"from azure.ai.textanalytics import (TextAnalyticsClient,ExtractSummaryAction) \r\n",
					"import openai\r\n",
					"import time"
				],
				"execution_count": 317
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create function to pull data from the Dedicated SQL Pool\r\n",
					"def QueryDataSQLPool (query):\r\n",
					"    # Name of the SQL Dedicated Pool or database where to run the query\r\n",
					"    # Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
					"    spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
					"\r\n",
					"    # Read from a query\r\n",
					"    # Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
					"    df = (spark.read\r\n",
					"                        # Name of the SQL Dedicated Pool or database where to run the query\r\n",
					"                        # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
					"                        .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
					"                        # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
					"                        # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
					"                        .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
					"                        # Defaults to storage path defined in the runtime configurations\r\n",
					"                        .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
					"                        # query from which data will be read\r\n",
					"                        .option(Constants.QUERY, query)\r\n",
					"                        .synapsesql()\r\n",
					"    )\r\n",
					"    return(df)"
				],
				"execution_count": 318
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Enter Cognitive Services credentials\r\n",
					"key = \"a3028b5f7e7a462d8a3381e49b9c976d\"\r\n",
					"endpoint = \"https://pendragon-language.cognitiveservices.azure.com/\""
				],
				"execution_count": 319
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Enter Azure OpenAI API credentials \r\n",
					"openai.api_type = \"azure\"\r\n",
					"openai.api_base = \"https://pendragonopenai.openai.azure.com/\"\r\n",
					"openai.api_version = \"2022-12-01\"\r\n",
					"openai.api_key = \"7c4b192d51f64c09a2ca680590ccae3f\""
				],
				"execution_count": 338
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define regular expressions for special characters, URLs, hashtags, mentions, and emojis\r\n",
					"special_chars_regex = r'[^\\w\\s]'\r\n",
					"url_regex = r'https?://\\S+'\r\n",
					"hashtag_regex = r'#\\w+'\r\n",
					"mention_regex = r'@\\w+'\r\n",
					"emoji_regex = r'[^\\w\\s\\ufe0f]+'\r\n",
					"\r\n",
					"# Define the UDF as a Spark SQL function\r\n",
					"clean_text_udf = udf(clean_text, StringType())"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define a UDF (user-defined function) to apply the regular expressions and stop words removal to each row of the dataframe\r\n",
					"def clean_text(text):\r\n",
					"    # Remove URLs, hashtags, mentions, and emojis using regular expressions\r\n",
					"    text = re.sub(url_regex, '', text)\r\n",
					"    text = re.sub(hashtag_regex, '', text)\r\n",
					"    text = re.sub(mention_regex, '', text)\r\n",
					"    text = re.sub(emoji_regex, '', text)\r\n",
					"\r\n",
					"    # Remove special characters\r\n",
					"    text = re.sub(special_chars_regex, '', text)\r\n",
					"\r\n",
					"    return text"
				],
				"execution_count": 321
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import nltk\r\n",
					"nltk.download('punkt')\r\n",
					"import re\r\n",
					"from nltk.corpus import stopwords\r\n",
					"nltk.download('stopwords')\r\n",
					"\r\n",
					"# Define a function to clean the text\r\n",
					"def clean_text1(text):\r\n",
					"    # Convert text to lowercase\r\n",
					"    text = text.lower()\r\n",
					"    # Remove URLs\r\n",
					"    text = re.sub(r'http\\S+', '', text)\r\n",
					"    # Remove mentions and hashtags\r\n",
					"    text = re.sub(r'@\\w+|#\\w+', '', text)\r\n",
					"    # Remove special characters\r\n",
					"    text = re.sub(r'[^\\w\\s]', '', text)\r\n",
					"    # Tokenize the text\r\n",
					"    tokens = nltk.word_tokenize(text)\r\n",
					"    # Remove stop words\r\n",
					"    stopwords_list = stopwords.words('english')\r\n",
					"    tokens = [token for token in tokens if token not in stopwords_list]\r\n",
					"    # Remove any remaining noise\r\n",
					"    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\r\n",
					"    return ' '.join(tokens)\r\n",
					"\r\n",
					"# Define a UDF to apply the clean_text function\r\n",
					"clean_text1_udf = udf(clean_text1, StringType())"
				],
				"execution_count": 352
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"clean_text('Are the people of Belarus so stupid that they cant see Putiins plan of putting nuclear weapons in ttheir country Putin hopes that a response by NATO would be to attack Belarus and not attack Russia How much more stupid can he get He must be executed fast. Russia has warned that the entrance of Sweden and Finland to NATO will make them legitimate targets for Russia. The threat against Sweden and Finland is seen as Russias latest attempt at a show of strength to the West after it was reported that the Kremlin was staging major nuclear missile exercises JoeBiden has literally has destabilizing Europe. LATEST Turkish Parliament to vote on Finlands bid to join NATO on March 30 The outcome of the vote could have major implications for the region. Swedish government says it will summon Russian ambassador in Stockholm over his statement saying that by joining NATO Finland and Sweden are legitimate targets for Russia. It is worth keeping in mind that NATO membership does not make Finland bigger than itself Membership brings tangible security but it is not allencompassing Finnish strong will is still needed Both the will to live in peace amp the will to hold on to our own Niinistö. This is the owner of the NATOTrans bedroom thats been floating around since the Nashville shooting A Starbuckscommunist black hole that is proud of being on welfare Their followers are a mix of communists Ukraine Nazi supporters Pron degenerates and Zionists In. Stockholm said today that it was summoning the Russian ambassador to Sweden for warning the country would become a legitimate target of retaliatory measures if it joined NATO Sweden and Finland decided to join the Western defense alliance in the wake of Russias invasion. USIP There is no vacuum in geopolitics says If the European Union and NATO are not present in an area that area will be penetrated by others or by third parties and in the case of it was he says. A NATO country should under no circumstances cooperate with Russia and in this perspective of cooperation with an aggressor country whose enemy is NATO Hungary should not prevent Swedens NATO membership. That is exactly what Putin Russia wanted from start of the new Cold War in 2007 NATO US nuclear expansion to the east US nukes threat in Ukraine is exactly the original Russian reason of invasion of Ukraine in 2014 and again in 2022.')"
				],
				"execution_count": 349
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create function to split input to Cogntive Services Extractive Summarization into batches of 125000 characters max\r\n",
					"def split_into_batches(input_string, max_chars_per_batch):\r\n",
					"    current_batch = \"\"\r\n",
					"    result = []\r\n",
					"    for c in input_string:\r\n",
					"        if len(current_batch) + 1 > max_chars_per_batch:\r\n",
					"            result.append(current_batch)\r\n",
					"            current_batch = c\r\n",
					"        else:\r\n",
					"            current_batch += c\r\n",
					"    if current_batch:\r\n",
					"        result.append(current_batch)\r\n",
					"    return result"
				],
				"execution_count": 323
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create function to authenticate the client using your key and endpoint \r\n",
					"def authenticate_client():\r\n",
					"    ta_credential = AzureKeyCredential(key)\r\n",
					"    text_analytics_client = TextAnalyticsClient(\r\n",
					"            endpoint=endpoint, \r\n",
					"            credential=ta_credential)\r\n",
					"    return text_analytics_client\r\n",
					"\r\n",
					"client = authenticate_client()"
				],
				"execution_count": 339
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Creat function to apply Exractive Summarization\r\n",
					"def sample_extractive_summarization(client,doc):\r\n",
					"\r\n",
					"    document = doc\r\n",
					"\r\n",
					"    poller = client.begin_analyze_actions(\r\n",
					"        document,\r\n",
					"        actions=[\r\n",
					"            ExtractSummaryAction(max_sentence_count=10)\r\n",
					"        ],\r\n",
					"    )\r\n",
					"    document_results = poller.result()\r\n",
					"    for result in document_results:\r\n",
					"        extract_summary_result = result[0]  # first document, first result\r\n",
					"        #if extract_summary_result.is_error:\r\n",
					"        #    print(\"...Is an error with code '{}' and message '{}'\".format(\r\n",
					"        #        extract_summary_result.code, extract_summary_result.message\r\n",
					"        #    ))\r\n",
					"        #else:\r\n",
					"            #print(\"Summary extracted: \\n{}\".format(\r\n",
					"            #    \" \".join([sentence.text for sentence in extract_summary_result.sentences]))\r\n",
					"            #)\r\n",
					"    return(\" \".join([sentence.text for sentence in extract_summary_result.sentences]))"
				],
				"execution_count": 325
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define dataframe schema\r\n",
					"schema = StructType([\r\n",
					"    StructField(\"topic\", ShortType(), True),\r\n",
					"    StructField(\"abstractive_summary\", StringType(), True)\r\n",
					"    ])"
				],
				"execution_count": 326
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Cognitive Services Extractive to OpenAI Abstractive Summarization"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def Abstractive_Summarization(dataframe, topic):\r\n",
					"    # Apply the UDF to the 'text' column of the dataframe and store the result in a new column named 'cleanText'\r\n",
					"    df = dataframe.withColumn('cleanText', clean_text_udf('text'))\r\n",
					"    # Apply the UDF1 to a DataFrame column\r\n",
					"    df = df.withColumn('cleanText', clean_text1_udf('cleanText'))\r\n",
					"    \r\n",
					"    # Remove empty spaces equal to or bigger than 2 spaces\r\n",
					"    df = df.withColumn('text_trimmed', regexp_replace(df['cleanText'], '\\s{2,}', ' '))\r\n",
					"    # Remove empty spaces before and after text.\r\n",
					"    df = df.withColumn('text_trimmed', trim(df['text_trimmed']))\r\n",
					"    text_col = df.select('text_trimmed')\r\n",
					"    # Collect all tweets into a list\r\n",
					"    text_list = text_col.rdd.map(lambda row: row[0]).collect()\r\n",
					"    # Join all tweets into a single document\r\n",
					"    result = \" \".join([text + \".\" for text in text_list])\r\n",
					"    document = [result]\r\n",
					"    # Extract the string that contains all tweets\r\n",
					"    input_string = document[0]\r\n",
					"\r\n",
					"    # Split the string into batches of 125000 characters maximum\r\n",
					"    max_chars_per_batch = 125000\r\n",
					"    result = split_into_batches(input_string, max_chars_per_batch)\r\n",
					"    print('Number of batches',len(result))\r\n",
					"    for batch in result:\r\n",
					"        print('Characters in batch', len(batch))\r\n",
					"\r\n",
					"    # For each batch create an extractive summary of 10 sentences\r\n",
					"    ex_summaries = []\r\n",
					"\r\n",
					"    for each in result:\r\n",
					"        #print('Batch', [each])\r\n",
					"        ex_summary = sample_extractive_summarization(client,[each])\r\n",
					"        ex_summaries.append([ex_summary])\r\n",
					"\r\n",
					"    print('All Extractive Summaries', ex_summaries) \r\n",
					"\r\n",
					"    ab_summaries = []\r\n",
					"\r\n",
					"    retries = 3\r\n",
					"    delay = 300\r\n",
					"\r\n",
					"    for i in range(retries):\r\n",
					"        try:\r\n",
					"            # Create a list of abstractive summaries for each extractive summary\r\n",
					"            for each in ex_summaries:\r\n",
					"                print(each)\r\n",
					"                print(each[0])\r\n",
					"                print(type(each[0]))\r\n",
					"                p = 'Provide a summary of the text below that captures its main idea.\\n\\n' + each[0] \r\n",
					"                print(p)\r\n",
					"\r\n",
					"                response = openai.Completion.create(\r\n",
					"                engine=\"SummaryDivinci003\",\r\n",
					"                prompt = p,\r\n",
					"                temperature=0.3,\r\n",
					"                max_tokens=250,\r\n",
					"                top_p=1,\r\n",
					"                frequency_penalty=0,\r\n",
					"                presence_penalty=0,\r\n",
					"                best_of=1,\r\n",
					"                stop=None)\r\n",
					"\r\n",
					"                print(response['choices'][0]['text'])\r\n",
					"\r\n",
					"                ab_summaries.append(response['choices'][0]['text'])\r\n",
					"                if len(ex_summaries) > 1:\r\n",
					"                    time.sleep(10)\r\n",
					"            print(ab_summaries)\r\n",
					"            print(\"Code succeeded!\")\r\n",
					"            break # Break out of the loop if code succeeds\r\n",
					"        except:\r\n",
					"            time.sleep(delay)\r\n",
					"            print('Failed')\r\n",
					"            #raise Exception(f\"Failed after {retries} retries\")\r\n",
					"            ab_summaries = []\r\n",
					"\r\n",
					"    else:\r\n",
					"        raise Exception(f\"Failed after {retries} retries\")\r\n",
					"\r\n",
					"    # Wait 10 seconds\r\n",
					"    time.sleep(10)\r\n",
					"\r\n",
					"    # Create one abstractive summary using the collection of abstractive summaries created above\r\n",
					"    p = 'Provide a summary of the text below that captures the main idea. ' + ''.join(ab_summaries)\r\n",
					"\r\n",
					"    response = openai.Completion.create(\r\n",
					"    engine=\"SummaryDivinci003\",\r\n",
					"    prompt = p,\r\n",
					"    temperature=0.3,\r\n",
					"    max_tokens=250,\r\n",
					"    top_p=1,\r\n",
					"    frequency_penalty=0,\r\n",
					"    presence_penalty=0,\r\n",
					"    best_of=1,\r\n",
					"    stop=None)\r\n",
					" \r\n",
					"    final_ab_summarization = response['choices'][0]['text']\r\n",
					"    #print(response['choices'][0]['text'])\r\n",
					"    print('-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\r\n",
					"    print('Abstractive Summary: ', final_ab_summarization)\r\n",
					"    print('Character Length:', len(final_ab_summarization))\r\n",
					"\r\n",
					"    # Put the meta abstractive summary into a data frame and return the data frame object \r\n",
					"    return(spark.createDataFrame([(topic, final_ab_summarization)], schema))\r\n",
					""
				],
				"execution_count": 353
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Topic 0 Summary"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_topic0 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 0\")\r\n",
					"df_topic0.count()"
				],
				"execution_count": 328
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"topic0_summary = Abstractive_Summarization(dataframe = df_topic0, topic=0)\r\n",
					"display(topic0_summary)"
				],
				"execution_count": 329
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"time.sleep(180)"
				],
				"execution_count": 330
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Topic 1 Summary"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_topic1 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 1\")\r\n",
					"df_topic1.count()"
				],
				"execution_count": 331
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"topic1_summary = Abstractive_Summarization(dataframe = df_topic1, topic=1)\r\n",
					"display(topic1_summary)"
				],
				"execution_count": 332
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"time.sleep(180)"
				],
				"execution_count": 333
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_topic2 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 2\")\r\n",
					"df_topic2.count()"
				],
				"execution_count": 334
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"topic2_summary = Abstractive_Summarization(dataframe = df_topic2, topic=2)\r\n",
					"display(topic2_summary)"
				],
				"execution_count": 354
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"time.sleep(180)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Topic 3 Summary"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_topic3 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 3\")\r\n",
					"df_topic3.count()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"topic3_summary = Abstractive_Summarization(dataframe = df_topic3, topic=3)\r\n",
					"display(topic3_summary)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"time.sleep(180)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Topic 4 Summary"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_topic4 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 4\")\r\n",
					"df_topic4.count()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"topic4_summary = Abstractive_Summarization(dataframe = df_topic4, topic=4)\r\n",
					"display(topic4_summary)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Union dataframes"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# union the dataframes\r\n",
					"Topic_Summary_df = topic0_summary.union(topic1_summary).union(topic2_summary).union(topic3_summary).union(topic4_summary)\r\n",
					"display(Topic_Summary_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Write Spark Dataframe to Dedicated SQL Pool Table [sum].[NATO_Abstractive_Sum]\r\n",
					"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
					"~~~\r\n",
					"CREATE TABLE [sum].[NATO_Abstractive_Sum]\r\n",
					"(\r\n",
					"    [abstractive_summary] NVARCHAR(4000) NULL,\r\n",
					"    [topic] SMALLINT NULL\r\n",
					")\r\n",
					"\r\n",
					"GO"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Write using AAD Auth to internal table\r\n",
					"# Add required imports\r\n",
					"import com.microsoft.spark.sqlanalytics\r\n",
					"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
					"\r\n",
					"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
					"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
					"(Topic_Summary_df.write\r\n",
					" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
					" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
					" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
					" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
					" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
					" # Choose a save mode that is apt for your use case.\r\n",
					" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
					" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
					" .mode(\"overwrite\")\r\n",
					" # Required parameter - Three-part table name to which data will be written\r\n",
					" .synapsesql(\"SQLPoolTest.sum.NATO_Abstractive_Sum\"))"
				],
				"execution_count": null
			}
		]
	}
}