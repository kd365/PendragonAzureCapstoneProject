{
	"name": "TopicModeling_LDA_Sklearn_UsingParms",
	"properties": {
		"folder": {
			"name": "NLP"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SparkPoolSmall",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "e5c08f69-e6ae-4fb5-84d8-45ef046412b8"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
				"name": "SparkPoolSmall",
				"type": "Spark",
				"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Perform Topic Modeling using Spark Machine Learning Clustering LDA Model\r\n",
					"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.LDA.html"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Add required imports\r\n",
					"import com.microsoft.spark.sqlanalytics\r\n",
					"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
					"from pyspark.sql.functions import col\r\n",
					"\r\n",
					"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
					"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
					"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
					"\r\n",
					""
				],
				"execution_count": 83
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create a variable for the output SQL Pool table name\r\n",
					"Enter a default place holder name which will be changed by the pipeline"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"topictable = \"Topic.NATO_Topics\"\r\n",
					"wordtable = \"Words.NATO_Topic_Words\"\r\n",
					"maintable = \"dbo.NATO_Tweets1\""
				],
				"execution_count": 84
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import *\r\n",
					"from pyspark.sql.types import *"
				],
				"execution_count": 86
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": []
				},
				"source": [
					"# Read from a query\n",
					"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\n",
					"df = (spark.read\n",
					"                     # Name of the SQL Dedicated Pool or database where to run the query\n",
					"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\n",
					"                     .option(Constants.DATABASE, \"SQLPoolTest\")\n",
					"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\n",
					"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\n",
					"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\n",
					"                     # Defaults to storage path defined in the runtime configurations\n",
					"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\n",
					"                     # query from which data will be read\n",
					"                     .option(Constants.QUERY, \"select * from \"+maintable)\n",
					"                     .synapsesql()\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Sort by created_at and sample if needed"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Select sample size for testing\r\n",
					"\r\n",
					"# Sort ascending by 'created_at'\r\n",
					"from pyspark.sql.functions import desc\r\n",
					"\r\n",
					"df = df.orderBy(desc(\"created_at\"))\r\n",
					"\r\n",
					"# select the first 100 records\r\n",
					"#df = df.limit(100)"
				],
				"execution_count": 88
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Clean data\r\n",
					"Remove special characters, URLs, hashtags, mentions, emojis, and stopwords"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import re\r\n",
					"from pyspark.sql.functions import udf\r\n",
					"from pyspark.sql.types import StringType\r\n",
					"\r\n",
					"# Define regular expressions for special characters, URLs, hashtags, mentions, and emojis\r\n",
					"special_chars_regex = r'[^\\w\\s]'\r\n",
					"url_regex = r'https?://\\S+'\r\n",
					"mentions_hashtag_regex = r'@\\w+|#\\w+'\r\n",
					"emoji_regex = r'[^\\w\\s\\ufe0f]+'\r\n",
					"extra_spaces_regex = r'\\s{2,}'\r\n",
					"\r\n",
					"def clean_text(text):\r\n",
					"    # Convert text to lowercase\r\n",
					"    text = text.lower()\r\n",
					"    # Remove URLs\r\n",
					"    text = re.sub(url_regex, '', text)\r\n",
					"    # Remove mentions and hashtags\r\n",
					"    text = re.sub(mentions_hashtag_regex, '', text)\r\n",
					"    # Remove special characters\r\n",
					"    text = re.sub(special_chars_regex, '', text)\r\n",
					"    # Remove 'amp'\r\n",
					"    text = re.sub(\"amp\", '', text)\r\n",
					"    # Remove emojis\r\n",
					"    text = re.sub(emoji_regex, '', text)\r\n",
					"    # Remove empty spaces equal to or bigger than 2 spaces\r\n",
					"    text = re.sub(extra_spaces_regex, ' ', text)\r\n",
					"    # Remove empty spaces\r\n",
					"    text = ' '.join(text.split())\r\n",
					"    return text\r\n",
					"\r\n",
					"# Apply the function to the \"text\" column of the DataFrame\r\n",
					"\r\n",
					"clean_text_udf = udf(clean_text, StringType())\r\n",
					"\r\n",
					"df = df.withColumn(\"cleanText\", clean_text_udf(\"text\"))\r\n",
					"\r\n",
					"#display(df)"
				],
				"execution_count": 89
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import split\r\n",
					"from pyspark.ml.feature import StopWordsRemover\r\n",
					"\r\n",
					"# split the \"text\" column into an array of words and create a new column \"words\"\r\n",
					"df = df.withColumn(\"words\", split(df[\"cleanText\"], \" \"))\r\n",
					"\r\n",
					"# Remove stopwords from the words array\r\n",
					"remover = StopWordsRemover(inputCol=\"words\", outputCol=\"removed\")\r\n",
					"df_removed = remover.transform(df)\r\n",
					"\r\n",
					"# concatenate the words in the \"words\" column into a single string and create a new column \"text_concat\"\r\n",
					"df_removed = df_removed.withColumn(\"cleanText\", concat_ws(\" \", df_removed[\"removed\"]))\r\n",
					"\r\n",
					"# drop the \"words\" column\r\n",
					"df_removed = df_removed.drop(\"words\")\r\n",
					"df_removed = df_removed.drop(\"removed\")\r\n",
					"\r\n",
					"display(df_removed)"
				],
				"execution_count": 90
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Perform Topic Modeling\r\n",
					"## Sklearn LDA (Latent Dirichlet Allocation)\r\n",
					"## 5 Topics"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from sklearn.decomposition import LatentDirichletAllocation\r\n",
					"from sklearn.feature_extraction.text import CountVectorizer\r\n",
					"import pandas as pd\r\n",
					"\r\n",
					"# Convert Spark DataFrame to Pandas DataFrame\r\n",
					"pandas_df = df_removed.toPandas()"
				],
				"execution_count": 91
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Tokenize and vectorize the text data\r\n",
					"count_vectorizer = CountVectorizer()\r\n",
					"count_data = count_vectorizer.fit_transform(pandas_df[\"cleanText\"])\r\n",
					"\r\n",
					"# Use LatentDirichletAllocation on Pandas DataFrame\r\n",
					"lda = LatentDirichletAllocation(n_components=5, random_state=42)\r\n",
					"lda.fit(count_data)\r\n",
					"lda_output = lda.transform(count_data)\r\n",
					"\r\n",
					"# Convert output to Pandas DataFrame and merge with original data\r\n",
					"lda_output_df = pd.DataFrame(lda_output, columns=[\"topic_0_confidence\", \"topic_1_confidence\",\"topic_2_confidence\",\"topic_3_confidence\",\"topic_4_confidence\"])\r\n",
					"pandas_df = pd.concat([pandas_df, lda_output_df], axis=1)\r\n",
					"\r\n",
					"pandas_df['topic'] = lda_output.argmax(axis=1)\r\n",
					"\r\n",
					"# Convert Pandas DataFrame back to Spark DataFrame\r\n",
					"spark_df = spark.createDataFrame(pandas_df)\r\n",
					"\r\n",
					"# Renaming the \"cleanText\" column to \"cleantext\"\r\n",
					"spark_df = spark_df.withColumnRenamed(\"cleanText\", \"cleantext\")\r\n",
					"\r\n",
					"# Display Spark DataFrame\r\n",
					"display(spark_df)"
				],
				"execution_count": 92
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Top 10 Word for each Topic"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# View the highest probability words for topics\r\n",
					"feature_names = count_vectorizer.get_feature_names_out()\r\n",
					"\r\n",
					"# Get the list of topic-term distributions from the LatentDirichletAllocation object\r\n",
					"topic_term_distributions = lda.components_\r\n",
					"\r\n",
					"# Get the top N most common words for each topic\r\n",
					"N = 10\r\n",
					"top_N_words = [[feature_names[i] for i in topic.argsort()[:-N - 1:-1]] for topic in topic_term_distributions]\r\n",
					"\r\n",
					"print(top_N_words)"
				],
				"execution_count": 93
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Create Pandas Dataframe for the top 10 words for each topic\r\n",
					"df = pd.DataFrame(top_N_words, columns=['word_'+str(i+1) for i in range(len(top_N_words[0]))])\r\n",
					"df.index.name = 'topic'\r\n",
					"\r\n",
					"# reset index and move Topic Number to a column\r\n",
					"df.reset_index(inplace=True)\r\n",
					"df.rename(columns={'index':'topic'}, inplace=True)\r\n",
					"\r\n",
					"# convert pandas dataframe to spark dataframe\r\n",
					"df_words = spark.createDataFrame(df)\r\n",
					"\r\n",
					"# display the spark dataframe\r\n",
					"display(df_words)"
				],
				"execution_count": 94
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark_df.printSchema"
				],
				"execution_count": 95
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Write Spark Dataframe to Dedicated SQL Pool Table [Topic].[NATO_Topics]\r\n",
					"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
					"~~~\r\n",
					"CREATE TABLE [Topic].[NATO_Topics]\r\n",
					"( \r\n",
					"\t[id] bigint  NULL,\r\n",
					"\t[created_at] DATETIME2(7)  NULL,\r\n",
					"\t[text] NVARCHAR(4000)  NULL,\r\n",
					"\t[lang] nvarchar(10)  NULL,\r\n",
					"\t[retweet_count] INT  NULL,\r\n",
					"\t[reply_count] INT  NULL,\r\n",
					"\t[like_count] INT  NULL,\r\n",
					"\t[quote_count] INT  NULL,\r\n",
					"\t[impression_count] INT  NULL,\r\n",
					"\t[cleantext] NVARCHAR(4000)  NULL,\r\n",
					"    [topic_0_confidence] FLOAT NULL,\r\n",
					"    [topic_1_confidence] FLOAT NULL,\r\n",
					"    [topic_2_confidence] FLOAT NULL,\r\n",
					"    [topic_3_confidence] FLOAT NULL,\r\n",
					"    [topic_4_confidence] FLOAT NULL,\r\n",
					"\t[topic] BIGINT NULL\r\n",
					")\r\n",
					"\r\n",
					"GO\r\n",
					"\r\n",
					"CREATE TABLE [Words].[NATO_Topic_Words]\r\n",
					"( \r\n",
					"\t[topic] bigint  NULL,\r\n",
					"\t[word_1] NVARCHAR(20)  NULL,\r\n",
					"\t[word_2] NVARCHAR(20)  NULL,\r\n",
					"    [word_3] NVARCHAR(20)  NULL,\r\n",
					"    [word_4] NVARCHAR(20)  NULL,\r\n",
					"    [word_5] NVARCHAR(20)  NULL,\r\n",
					"    [word_6] NVARCHAR(20)  NULL,\r\n",
					"    [word_7] NVARCHAR(20)  NULL,\r\n",
					"    [word_8] NVARCHAR(20)  NULL,\r\n",
					"    [word_9] NVARCHAR(20)  NULL,\r\n",
					"    [word_10] NVARCHAR(20)  NULL\r\n",
					")\r\n",
					"\r\n",
					"GO"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Create the three-part table name to which data will be written"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"outputtopictable = \"SQLPoolTest.\" + topictable\r\n",
					"outputwordtable = \"SQLPoolTest.\" + wordtable"
				],
				"execution_count": 103
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Write using AAD Auth to internal table\r\n",
					"# Add required imports\r\n",
					"import com.microsoft.spark.sqlanalytics\r\n",
					"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
					"\r\n",
					"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
					"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
					"(spark_df.write\r\n",
					" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
					" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
					" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
					" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
					" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
					" # Choose a save mode that is apt for your use case.\r\n",
					" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
					" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
					" .mode(\"overwrite\")\r\n",
					" # Required parameter - Three-part table name to which data will be written\r\n",
					" .synapsesql(outputtopictable))"
				],
				"execution_count": 98
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_words.printSchema"
				],
				"execution_count": 99
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Write using AAD Auth to internal table\r\n",
					"# Add required imports\r\n",
					"import com.microsoft.spark.sqlanalytics\r\n",
					"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
					"\r\n",
					"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
					"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
					"(df_words.write\r\n",
					" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
					" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
					" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
					" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
					" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
					" # Choose a save mode that is apt for your use case.\r\n",
					" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
					" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
					" .mode(\"overwrite\")\r\n",
					" # Required parameter - Three-part table name to which data will be written\r\n",
					" .synapsesql(outputwordtable))"
				],
				"execution_count": 104
			}
		]
	}
}